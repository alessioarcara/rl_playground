{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Space: Discrete(4)\n",
      "Observation Space: Dict('agent': Box(0, 9, (2,), int64), 'goal': Box(0, 9, (2,), int64))\n",
      "Max Episode Steps: None\n",
      "Nondeterministic: False\n",
      "Reward Threshold: None\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "\n",
    "def query_environment(name):\n",
    "    env = gym.make(name)\n",
    "    spec = gym.spec(name)\n",
    "    print(f\"Action Space: {env.action_space}\")\n",
    "    print(f\"Observation Space: {env.observation_space}\")\n",
    "    print(f\"Max Episode Steps: {spec.max_episode_steps}\")\n",
    "    print(f\"Nondeterministic: {spec.nondeterministic}\")\n",
    "    print(f\"Reward Threshold: {spec.reward_threshold}\")\n",
    "\n",
    "\n",
    "query_environment(\"GridWorld-v0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bisogna fare una funzione chiamata `train_dqn_agent`, in cui:\n",
    "- la funzione riceve due parametri `num_episodes` e `grid_size`\n",
    "- inizialmente istanzia l'`env` e l'`agent`\n",
    "- ad ogni episodio resetta l'`env`\n",
    "- per ogni episodio fino che non ha finito (stato terminale):\n",
    "    - l'`agent` seleziona un azione tramite epsilon greedy\n",
    "    - l'`agent` ha la seguente funzione `step(s,a,r,s',a')` che viene chiamata per calcolare la loss, calcolare il gradiente e aggiornare la `q_network`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/aarcara/rl_playground/wandb/run-20250503_143742-yypqkyxb</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/IPCV_ASS2/rl-playground/runs/yypqkyxb' target=\"_blank\">glamorous-tree-54</a></strong> to <a href='https://wandb.ai/IPCV_ASS2/rl-playground' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/IPCV_ASS2/rl-playground' target=\"_blank\">https://wandb.ai/IPCV_ASS2/rl-playground</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/IPCV_ASS2/rl-playground/runs/yypqkyxb' target=\"_blank\">https://wandb.ai/IPCV_ASS2/rl-playground/runs/yypqkyxb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-03 14:37:42.711\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain_dql_agent\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mStarting training with config: {'batch_size': 32, 'lr': 0.001, 'gamma': 0.99, 'eps_start': 1.0, 'eps_end': 0.05, 'hidden_dim': 128, 'n_episodes': 500, 'max_steps_per_episode': 32, 'grid_size': 5, 'seed': 0, 'target_update_frequency': 4}\u001b[0m\n",
      "\u001b[32m2025-05-03 14:37:42.711\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain_dql_agent\u001b[0m:\u001b[36m39\u001b[0m - \u001b[34m\u001b[1mState dim: 50, Action dim: 4\u001b[0m\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "<class 'wandb.sdk.wandb_config.Config'> object has no attribute 'max_n_steps'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rl/lib/python3.11/site-packages/wandb/sdk/wandb_config.py:165\u001b[39m, in \u001b[36mConfig.__getattr__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m    164\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m165\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[34;43m__getitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    166\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ke:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rl/lib/python3.11/site-packages/wandb/sdk/wandb_config.py:130\u001b[39m, in \u001b[36mConfig.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m    129\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_items\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[31mKeyError\u001b[39m: 'max_n_steps'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 77\u001b[39m\n\u001b[32m     73\u001b[39m     env.close()\n\u001b[32m     74\u001b[39m     wandb.finish()\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m \u001b[43mtrain_dql_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 41\u001b[39m, in \u001b[36mtrain_dql_agent\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     38\u001b[39m action_dim = env.action_space.n\n\u001b[32m     39\u001b[39m logger.debug(\u001b[33m\"\u001b[39m\u001b[33mState dim: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m, Action dim: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m\"\u001b[39m, state_dim, action_dim)\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m decay_steps = \u001b[38;5;28mint\u001b[39m(config.n_episodes * \u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmax_n_steps\u001b[49m)\n\u001b[32m     42\u001b[39m agent = DeepQLearningAgent(config, state_dim, action_dim, decay_steps)\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m ep \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, config.n_episodes + \u001b[32m1\u001b[39m), desc=\u001b[33m\"\u001b[39m\u001b[33mEpisodes\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rl/lib/python3.11/site-packages/wandb/sdk/wandb_config.py:167\u001b[39m, in \u001b[36mConfig.__getattr__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m    165\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.\u001b[34m__getitem__\u001b[39m(key)\n\u001b[32m    166\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ke:\n\u001b[32m--> \u001b[39m\u001b[32m167\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[32m    168\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m object has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    169\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mke\u001b[39;00m\n",
      "\u001b[31mAttributeError\u001b[39m: <class 'wandb.sdk.wandb_config.Config'> object has no attribute 'max_n_steps'"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import jax.nn as nn\n",
    "import jax.numpy as jnp\n",
    "from loguru import logger\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import src.gymnasium_env\n",
    "import wandb\n",
    "from src.agent import DeepQLearningAgent\n",
    "from src.config import init_wandb\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "def onehot_agent_goal_positions(agent, goal, grid_size=10):\n",
    "    N = grid_size * grid_size\n",
    "    agent_idx = jnp.ravel_multi_index((agent[1], agent[0]), (grid_size, grid_size))\n",
    "    goal_idx = jnp.ravel_multi_index((goal[1], goal[0]), (grid_size, grid_size))\n",
    "\n",
    "    oh_agent = nn.one_hot(agent_idx, N, dtype=jnp.int32)\n",
    "    oh_goal = nn.one_hot(goal_idx, N, dtype=jnp.int32)\n",
    "\n",
    "    return jnp.concatenate([oh_agent, oh_goal])\n",
    "\n",
    "\n",
    "def train_dql_agent():\n",
    "    config = init_wandb()\n",
    "    wandb.define_metric(\"train/episode_reward\", step_metric=\"episode\")\n",
    "    wandb.define_metric(\"epsilone\", step_metric=\"global_step\")\n",
    "\n",
    "    logger.info(\"Starting training with config: {}\", config)\n",
    "\n",
    "    grid_size = config.grid_size\n",
    "    env = gym.make(\"GridWorld-v0\", size=grid_size)\n",
    "\n",
    "    state_dim = 2 * grid_size * grid_size\n",
    "    action_dim = env.action_space.n\n",
    "    logger.debug(\"State dim: {}, Action dim: {}\", state_dim, action_dim)\n",
    "\n",
    "    decay_steps = int(config.n_episodes * config.max_steps_per_episode)\n",
    "    agent = DeepQLearningAgent(config, state_dim, action_dim, decay_steps)\n",
    "\n",
    "    for ep in tqdm(range(1, config.n_episodes + 1), desc=\"Episodes\"):\n",
    "        obs, _ = env.reset()\n",
    "        state = onehot_agent_goal_positions(obs[\"agent\"], obs[\"goal\"], grid_size)\n",
    "        done = False\n",
    "        ep_reward = 0.0\n",
    "\n",
    "        for _ in range(config.max_steps_per_episode):\n",
    "            action = agent.act(state)\n",
    "            next_obs, reward, done, _, _ = env.step(action)\n",
    "            next_state = onehot_agent_goal_positions(\n",
    "                next_obs[\"agent\"], next_obs[\"goal\"], grid_size\n",
    "            )\n",
    "\n",
    "            _ = agent.learn(state, action, reward, next_state, done)\n",
    "            ep_reward += reward\n",
    "            state = next_state\n",
    "\n",
    "            wandb.log({\"global_step\": agent.step, \"epsilon\": agent.eps})\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        wandb.log(\n",
    "            {\n",
    "                \"episode\": ep,\n",
    "                \"train/episode_reward\": ep_reward,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    env.close()\n",
    "    wandb.finish()\n",
    "\n",
    "\n",
    "train_dql_agent()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
