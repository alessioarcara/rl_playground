{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Space: Discrete(4)\n",
      "Observation Space: Dict('agent': Box(0, 9, (2,), int64), 'goal': Box(0, 9, (2,), int64))\n",
      "Max Episode Steps: None\n",
      "Nondeterministic: False\n",
      "Reward Threshold: None\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import src.gymnasium_env\n",
    "\n",
    "\n",
    "def query_environment(name):\n",
    "    env = gym.make(name)\n",
    "    spec = gym.spec(name)\n",
    "    print(f\"Action Space: {env.action_space}\")\n",
    "    print(f\"Observation Space: {env.observation_space}\")\n",
    "    print(f\"Max Episode Steps: {spec.max_episode_steps}\")\n",
    "    print(f\"Nondeterministic: {spec.nondeterministic}\")\n",
    "    print(f\"Reward Threshold: {spec.reward_threshold}\")\n",
    "\n",
    "\n",
    "query_environment(\"GridWorld-v0\")\n",
    "\n",
    "# observation, info = env.reset(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bisogna fare una funzione chiamata `train_dqn_agent`, in cui:\n",
    "- la funzione riceve due parametri `num_episodes` e `grid_size`\n",
    "- inizialmente istanzia l'`env` e l'`agent`\n",
    "- ad ogni episodio resetta l'`env`\n",
    "- per ogni episodio fino che non ha finito (stato terminale):\n",
    "    - l'`agent` seleziona un azione tramite epsilon greedy\n",
    "    - l'`agent` ha la seguente funzione `step(s,a,r,s',a')` che viene chiamata per calcolare la loss, calcolare il gradiente e aggiornare la `q_network`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array((), dtype=key<fry>) overlaying:\n",
      "[ 0 42]\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import random\n",
    "\n",
    "# random consumes the key but not modify it\n",
    "key = random.key(42)\n",
    "print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "draw 0: 0.6057640314102173\n",
      "draw 1: -0.21089035272598267\n",
      "draw 2: -0.3948981463909149\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    key, subkey = random.split(key)\n",
    "    val = random.normal(subkey)\n",
    "    print(f\"draw {i}: {val}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.07592554  0.60576403  0.4323065  -0.2818947   0.6549178  -0.2166012 ]\n",
      "vectorized: [ 0.07592554  0.60576403  0.4323065  -0.2818947   0.6549178  -0.2166012 ]\n"
     ]
    }
   ],
   "source": [
    "# no sequential guarantee\n",
    "import numpy as np\n",
    "\n",
    "key = random.key(42)\n",
    "subkeys = random.split(key, 6)\n",
    "print(np.stack([random.normal(subkey) for subkey in subkeys]))\n",
    "print(\"vectorized:\", jax.vmap(random.normal)(subkeys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aarcara/miniconda3/envs/rl/lib/python3.11/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  pid, fd = os.forkpty()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting loguru\n",
      "  Using cached loguru-0.7.3-py3-none-any.whl.metadata (22 kB)\n",
      "Using cached loguru-0.7.3-py3-none-any.whl (61 kB)\n",
      "Installing collected packages: loguru\n",
      "Successfully installed loguru-0.7.3\n"
     ]
    }
   ],
   "source": [
    "!pip install loguru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.3933988  -0.3365822   0.14810538 -0.15271175]\n"
     ]
    }
   ],
   "source": [
    "from flax import nnx\n",
    "import optax\n",
    "from loguru import logger\n",
    "from src.q_network import QNetwork\n",
    "\n",
    "\n",
    "rngs = nnx.Rngs(0)\n",
    "model = QNetwork(4, 128, 4, rngs=rngs)\n",
    "optimizer = nnx.Optimizer(model, optax.adam(1e-3))\n",
    "\n",
    "dummy_state = jnp.ones((4,), dtype=jnp.float32)\n",
    "\n",
    "\n",
    "# da sistemare\n",
    "def loss_fn(model: QNetwork, batch):\n",
    "    err = model(batch)\n",
    "    return jnp.mean(jnp.square(err))\n",
    "\n",
    "\n",
    "@nnx.jit\n",
    "def train_step(optim: nnx.Optimizer, batch):\n",
    "    loss, grads = nnx.value_and_grad(loss_fn)(optim.model, batch)\n",
    "    optim.update(grads)\n",
    "\n",
    "\n",
    "class DeepQLearningAgent:\n",
    "    def __init__(self, state_dim: int, action_dim: int, hidden_dim: int = 128, eps_start: float = 1.0):\n",
    "        self.eps = eps_start\n",
    "        q_network = QNetwork(state_dim, hidden_dim, action_dim, rngs=nnx.Rngs(42))\n",
    "        tx = optax.adam(1e-3)\n",
    "        self.state = nnx.Optimizer(q_network, tx)\n",
    "\n",
    "    def act(self) -> int:\n",
    "        # da sistemare\n",
    "        if random.random() < self.eps:\n",
    "            return np.argmax(self.Q)\n",
    "        else:\n",
    "            #q_vals = self.optim.model(jnp.)\n",
    "            return random.randint(0, len(self.Q) - 1)\n",
    "\n",
    "    def learn(self):\n",
    "        # da implementare\n",
    "        pass\n",
    "\n",
    "\n",
    "def train_dql_agent(num_episodes: int = 500, grid_size: int = 10):\n",
    "    env = gym.make(\"GridWorld-v0\", size=grid_size)\n",
    "    state_dim  = (\n",
    "        env.observation_space[\"agent\"].shape[0] +\n",
    "        env.observation_space[\"goal\"].shape[0]\n",
    "    )\n",
    "    action_dim = env.action_space.n\n",
    "    logger.debug(\"State dim: {}, Action dim: {}\", state_dim, action_dim)\n",
    "    agent = DeepQLearningAgent(state_dim, action_dim)\n",
    "\n",
    "    for _ in tqdm(range(num_episodes)):\n",
    "        state, _ = env.reset()\n",
    "        #state = preprocess(obs)\n",
    "\n",
    "        done = False\n",
    "        while not done:\n",
    "            #action = agent.act()\n",
    "            action = env.action_space.sample()\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "\n",
    "#train_dql_agent()\n",
    "\n",
    "\n",
    "# agent = DeepQLearningAgent()\n",
    "# agent.train_step(jnp.ones((3,)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
